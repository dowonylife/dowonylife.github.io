---
layout: post
title: "cs231n#1. The Image Classification Task & Linear Classification & kNN"
---

이미지를 분류할 때 다음과 같은 문제가 발생할 수 있음

## Problem: Semantic Gap
- **Challenges**:
  - Viewpoint variation
  - Illumination
  - Background Clutter
  - Occlusion
  - Deformation
  - Intraclass variation
  - Context

클래스를 구별하기 위한 명백한 알고리즘 방법 없음. 그중 하나로, 원본 이미지로부터 유용한 정보를 추출하기 위해 엣지와 코너를 검출하고, 방향성과 같은 특징을 추출함.

## 머신러닝
데이터 기반 접근 방식으로, 이미지 라벨 데이터를 수집하고 머신러닝 알고리즘을 통해 훈련하여 새로운 이미지의 구별 능력을 평가함.

## Nearest Neighbor
- `train()`: 데이터를 기억함. (O(1))
- `predict()`: 가장 가까운 훈련 데이터를 찾아서 라벨을 예측함. (O(n))

하지만 훈련은 늦어도 예측이 빠른 것이 좋음.

## K-최근접 이웃(K-Nearest Neighbors, K-NN)
가장 가까운 K개의 이웃 데이터를 참고하여 다수결 투표를 통해 클래스를 결정함.

- 다양한 거리 측정 방법이 있음: L1 거리(맨해튼 거리)와 L2 거리(유클리드 거리)
- 하이퍼파라미터: K값과 거리 측정 방법
- 데이터셋에 의존하지 말고 모든 것을 시험해보고 가장 효과적인 것을 확인해야 함

1. 훈련, 테스트 데이터셋에 적합한 하이퍼파라미터를 고르지 말고, 데이터를 `train`과 `val`로 분할해서 `val`에 대한 하이퍼파라미터를 선택함
2. 데이터를 나눠서 각각 검증하고 결과의 평균을 구하는 교차 검증(Cross-Validation)을 함
3. 최적의 K 값을 찾기 위해 교차 검증을 사용하면 모델의 성능을 최대화할 수 있음 (최적의 K 값은 데이터셋에 따라 다를 수 있음)

하지만 픽셀 단위의 거리 측정이 이미지 간의 의미 있는 유사성을 잘 반영하지 못할 수 있음
- 차폐된 이미지는 원본과는 다르지만, 일부만 변경되었기에 픽셀 간 거리가 크게 차이나지 않음
- 한 픽셀만 이동된 경우 원본과 유사하지만, 픽셀 단위 거리는 상당히 달라질 수 있음
- 색조 변경 역시 픽셀 단위 거리 측정에 큰 차이가 없음

픽셀 단위의 거리 측정은 이미지의 실제 의미나 구조를 잘 반영하지 못할 수 있음. 대신, 이미지의 의미와 구조를 더 잘 반영할 수 있는 특징(feature)을 사용한 거리 측정이 필요함. SIFT, SURF, HOG 등의 특징 추출 방법을 사용하여 이미지 간의 유사성을 측정하는 것이 더 효과적일 수 있음.

## Linear Classifier
`f(x, W) = Wx + b`로 구성됨. 예를 들어 10개의 클래스 점수가 있다면, `f(x, W)`는 10x1, `W`는 10x3072, `x`는 3072x1 (32x32x3), `b`는 10x1로 구성됨

**뉴럴 네트워크**는 여러 층의 뉴런이 쌓여 있는 구조로, 각 층은 선형 분류기와 유사한 역할을 하면서도 비선형성을 추가하여 더 복잡한 문제를 해결할 수 있음. 보통 linear layers는 신경망이 이미지나 다른 데이터에서 학습한 특징을 바탕으로 최종 결정을 내리는 부분

- 선형 분류기가 직선 하나로 데이터를 분류할 수 없는 비선형적인 데이터 분포도 많이 나타남
  - 비선형 분류기(예: 신경망, 서포트 벡터 머신(SVM) 등의 커널 트릭 사용)가 필요함. 다중 계층을 사용하거나 비선형 변환을 통해 데이터를 분류할 수 있음

- W 최적화: 우선 훈련 데이터 전체 점수에 대한 잘못된 값을 정량화하는 손실함수(loss function)를 정의함. 그리고 손실 함수를 최소화하기 위해 효율적으로 파라미터를 찾는 방법을 생각함
  - 멀티클래스 서포트 벡터 머신(SVM) 손실 함수: 정답 클래스의 점수가 다른 클래스의 점수보다 충분히 크도록 하는 손실을 계산함

## Softmax Classifier
원시 분류기 점수(raw classifier scores)를 확률로 해석하기 위해 소프트맥스 함수(Softmax function)를 사용함

- 각 클래스의 점수를 지수 함수로 변환한 후, 이들의 합으로 나누어 확률을 계산함
- 크로스 엔트로피 손실 함수를 사용하여 모델을 학습함 (`L = −logP`)
- 모델이 출력한 점수를 해석 가능한 확률로 변환하여 분류 문제 해결에 유용함
